{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1> Nearest Neighbor </h1>\n",
    "</center>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import fasttext\n",
    "\n",
    "from dataset     import shuffle_and_get_dataset, es_significativo\n",
    "from normalizer  import preprocesar_codigo, combinar_y_normalizar\n",
    "from utilitarios import get_enunciado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_to_txt(data, fname, normalizer):\n",
    "    \"\"\"\n",
    "    Preprocesa los datos y los guarda en un txt en un formato usable para FastText\n",
    "    para el entrenamiento no supervisado\n",
    "    Args:\n",
    "        data        :  Samples del dataset \n",
    "        fname       :  Nombre del archivo resultante\n",
    "        normalizer  :  Funcion de normalizacion de strings\n",
    "    \"\"\"\n",
    "    with open(fname, 'w') as file:\n",
    "        for i in range(len(data)):\n",
    "            code = normalizer(data[i])\n",
    "            file.write(f\"{code}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtencion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de datos\n",
    "discussions_df = pd.read_csv(\"./Data/discussions_anon.csv\")\n",
    "items_df       = pd.read_csv(\"./Data/items_anon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25056, 0, 6264, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filas del archivo items que corresponden al lenguaje JavaScript\n",
    "JS_ID = 5   # Identificador de codigo JavaScript dentro de la base de datos\n",
    "\n",
    "item_js    = items_df[items_df[\"language_id\"] == JS_ID]\n",
    "item_id_js = item_js[\"id\"]\n",
    "\n",
    "# Filas del archivo discussion que corresponden al lenguaje JavaScript\n",
    "discussion_js = discussions_df[discussions_df.item_id.isin(item_id_js)]\n",
    "\n",
    "# Datasets \n",
    "data   = discussion_js[[\"solution\", \n",
    "                        \"test_results\",\n",
    "                        \"expectation_results\", \n",
    "                        \"item_id\",\n",
    "                        \"id\"]].values.astype(str)\n",
    "\n",
    "target = discussion_js[\"submission_status\"].values.astype(str)\n",
    "\n",
    "\n",
    "# Hay dos etiquetas que no parecen significativas para la tarea: aborted y pending\n",
    "# por lo que las omitire de los datos\n",
    "mask = [sample not in [\"aborted\", \"pending\"] for sample in target]\n",
    "data   = data[mask]\n",
    "target = target[mask]\n",
    "\n",
    "# Obtenemos los datasets que usaremos para entrenar los modelos\n",
    "(train_data, train_target), (val_data, val_target), \\\n",
    "(test_data, test_target) = shuffle_and_get_dataset(data, target, \n",
    "                                                   val_prop=0.0, \n",
    "                                                   test_prop=0.2)\n",
    "\n",
    "len(train_data), len(val_data), len(test_data), len(np.unique(target))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrado de datos no significados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrar_no_significativo(data, target):\n",
    "    \"\"\"\n",
    "    Filtra las muestras no significativas y sus respectivas labels. Las muestras\n",
    "    del dataset se asumen que en su primera coordenada tienen codigo\n",
    "    Args:\n",
    "        data      :  Muestras del dataset\n",
    "        target    :  label para cada muestra\n",
    "    \"\"\"\n",
    "    # Filtramos los datos que no son informativos con un boolmap\n",
    "    boolmap = []\n",
    "    for code, label in zip(data[:, 0], target):\n",
    "        boolmap.append(es_significativo(code, label))\n",
    "        \n",
    "    # Datos no informativos filtrados\n",
    "    data   = data[boolmap]\n",
    "    target = target[boolmap]\n",
    "    \n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24691, 0, 6171)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_train_data, f_train_target  = filtrar_no_significativo(train_data, train_target)\n",
    "f_val_data  , f_val_target    = filtrar_no_significativo(val_data  , val_target)\n",
    "f_test_data , f_test_target   = filtrar_no_significativo(test_data , test_target)\n",
    "\n",
    "len(f_train_data), len(f_val_data), len(f_test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documento de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el archivo de entrenamiento de FastText\n",
    "dataset_dir = \"./FastText_Datasets/\" # Directorio de datasets\n",
    "preprocess_to_txt( f_train_data[:, 0:3], \n",
    "                   dataset_dir + \"clustering.txt\", \n",
    "                   combinar_y_normalizar )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicion del Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 30 \n",
    "wng       = 3\n",
    "epocas    = 15\n",
    "\n",
    "word_embedding = fasttext.train_unsupervised(\n",
    "    dataset_dir + \"clustering.txt\", # Dataset de entrenamiento \n",
    "    model = \"skipgram\",             # Tipo de modelo\n",
    "    dim=dimension,                  # Dimension del word embedding\n",
    "    wordNgrams=wng,                 # Tamaño del ngram\n",
    "    epoch=epocas,                   # Cantidad de epocas\n",
    "    ws=5,                           # Tamaño del context window\n",
    "    loss=\"ns\",                      # Negative sampling\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# <code, test_results, expectation_results> -> Vector\n",
    "# Usaremos estos vectores para asociar a una nueva muestra (las del conjunto de test)\n",
    "# codigos similares\n",
    "train_vectors = np.array([word_embedding.get_sentence_vector(combinar_y_normalizar(code)) for \\\n",
    "                          code in f_train_data[:, 0:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distancia euclidea entre dos puntos\n",
    "euclidean = lambda x, y: np.linalg.norm(x-y) \n",
    "\n",
    "def k_nearest_neighbors(k, sample, vectores):\n",
    "    \"\"\"\n",
    "    Retorna el indice de los k vectores mas cercanos al vector\n",
    "    del word embedding asociado a sample\n",
    "    Args:\n",
    "        k           :    Cantidad de indices a retornar \n",
    "        sample      :    Una muestra del conjunto de datos\n",
    "        vectores    :    Vectores de 'entrenamiento' producidos por el \n",
    "                         word embedding\n",
    "    \"\"\"\n",
    "    codigo_preprocesado = combinar_y_normalizar(sample)\n",
    "    codigo_vectorizado  = word_embedding.get_sentence_vector(codigo_preprocesado)\n",
    "\n",
    "    distancias = np.array([euclidean(codigo_vectorizado, v) for \\\n",
    "                           v in vectores])\n",
    "        \n",
    "    k_nn = distancias.argsort()[:k] # Indices de los K vectores mas cercanos \n",
    "    \n",
    "    return k_nn\n",
    "\n",
    "\n",
    "def print_k_nn(k, sample, id_enunciado, vectores):\n",
    "    \"\"\"\n",
    "    Imprime el codigo dado junto a su enunciado y a los \n",
    "    k codigos mas cercanos. Los codigos se imprimen sin normalizar, \n",
    "    por lo que aparecen tal cual como se escribieron. \n",
    "    Args:\n",
    "        k             :  Cantidad de codigos similares\n",
    "        sample        :  Muestra del conjunto de datos (<code, test_results, expectation_results>)\n",
    "        id_enunciado  :  Enunciado asociado a la muestra sample\n",
    "        vectores      :  Vectores de 'entrenamiento' producidos por el \n",
    "                         word embedding\n",
    "    \"\"\"\n",
    "    indices = k_nearest_neighbors(k, sample, vectores)\n",
    "    \n",
    "    print(get_enunciado(int(id_enunciado), item_js)) # Imprime el enunciado\n",
    "    print(\"CODIGO DADO\\n\")\n",
    "    print(sample[0])                                 # Imprime el codigo dado\n",
    "    print(f\"CODIGOS CERCANOS\\n\")   \n",
    "    for i in indices:                                # Imprime los codigos cercanos\n",
    "        print(f_train_data[i][0])\n",
    "        print(f\"\\n {'#'*80} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scripts de prueba\n",
    "\n",
    "El siguiente script toma una muestra aleatoria del conjunto de datos no vistos por el modelo, busca los k codigos mas cercanos y los imprime por pantalla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Operando strings - 2769] :: ¿Y qué podemos hacer con los strings, además de compararlos? ¡Varias cosas! Por ejemplo, podemos preguntarles cuál es su cantidad de letras:\n",
      "\n",
      "```javascript\n",
      "ム longitud(\"biblioteca\")\n",
      "10\n",
      "ム longitud(\"babel\")\n",
      "5\n",
      "```\n",
      "\n",
      "O también podemos _concatenarlos_, es decir, obtener **uno nuevo** que junta dos strings:\n",
      "\n",
      "```javascript\n",
      "ム \"aa\" + \"bb\"\n",
      "\"aabb\"\n",
      "ム \"sus anaqueles \" + \"registran todas las combinaciones\"\n",
      "\"sus anaqueles registran todas las combinaciones\"\n",
      "```\n",
      "\n",
      "O podemos preguntarles si uno comienza con otro:\n",
      "\n",
      "```javascript\n",
      "ム comienzaCon(\"una página\", \"una\")\n",
      "true\n",
      "ム comienzaCon(\"la biblioteca\", \"todos los fuegos\")\n",
      "false\n",
      "```\n",
      "\n",
      "> Veamos si queda claro: definí la función `longitudNombreCompleto`, que tome un nombre, un segundo nombre y un apellido, y retorne su longitud total, contando dos espacios extra para separarlos:\n",
      ">\n",
      ">```javascript\n",
      "> ム longitudNombreCompleto(\"Cosme\", \"Miguel\", \"Fulanito\")\n",
      ">21\n",
      ">```\n",
      "CODIGO DADO\n",
      "\n",
      "function longitudNombreCompleto (Nombre, Apellido) {\n",
      "  return longitud (\"Nombre\" + \"Apellido\") }\n",
      "CODIGOS CERCANOS\n",
      "\n",
      "function longitudNombreCompleto(nombre,apellido){\n",
      "  return  longitud(\"nombre\"+\"apellido\")\n",
      "}\n",
      "\n",
      " ################################################################################ \n",
      "\n",
      "function longitudNombreCompleto(nombre,apellido ){\n",
      "return longitud(\"nombre\"+\"apellido\")\n",
      "\n",
      "}\n",
      "\n",
      " ################################################################################ \n",
      "\n",
      " function longitudNombreCompleto(nombre, apellido) {\n",
      "   \n",
      "  return longitud (\"nombre\" + \"apellido\")\n",
      "\n",
      " }\n",
      "\n",
      "\n",
      " ################################################################################ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "desired_label = \"failed\"    # Para elegir una muestra aleatoria con ese submission_status\n",
    "\n",
    "# Eleccion de una muestra aleatoria con la label deseada\n",
    "choice = np.random.randint(len(f_test_data[f_test_target == desired_label]))\n",
    "muestra_elegida = f_test_data[f_test_target == desired_label][choice]\n",
    "\n",
    "print_k_nn(\n",
    "    k = 3,                                  # Cantidad de codigos cercanos a imprimir   \n",
    "    sample       = muestra_elegida[:3],     # <codigo, test_result, expectation_result>\n",
    "    id_enunciado = muestra_elegida[3],      # item_id de la muestra\n",
    "    vectores     = train_vectors            # vectores calculados por el word embedding\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otro script de prueba\n",
    "\n",
    "Este es completamente analogo al anterior pero ahora imprime la discussion_id de los ejercicios cercanos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id seleccionada: 239\n",
      "Ids de codigos cercanos:\n",
      "7928\n",
      "28834\n",
      "14211\n"
     ]
    }
   ],
   "source": [
    "desired_label = \"failed\"    # Para elegir una muestra aleatoria con ese submission_status\n",
    "\n",
    "# Eleccion de una muestra aleatoria con la label deseada\n",
    "choice = np.random.randint(len(f_test_data[f_test_target == desired_label]))\n",
    "muestra_elegida = f_test_data[f_test_target == desired_label][choice]\n",
    "\n",
    "indices = k_nearest_neighbors (\n",
    "    k = 3,                                  # Cantidad de codigos cercanos a imprimir  \n",
    "    sample       = muestra_elegida[:3],     # <codigo, test_result, expectation_result>\n",
    "    vectores     = train_vectors            # vectores calculados por el word embedding\n",
    ")\n",
    "\n",
    "print(f\"Id seleccionada: {muestra_elegida[4]}\")\n",
    "print(f\"Ids de codigos cercanos:\")\n",
    "for i in indices:\n",
    "    print(f_train_data[i][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
